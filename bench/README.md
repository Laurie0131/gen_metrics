## Benchmarks

Some of you may be curious about the performance impact `gen_metrics` has on the servers and pipelines it is observing, so we've put together a number of benchmarks to compare the overhead of traced vs untraced servers and pipelines. You can tweak and run the benchmarks yourself under the current directory. The following sections introduce each of the available benchmark tests and we examine and explain the implications of each result.

The benchmark reports that follow were generated by running the benchmarks on a 2011 Macbook Air (1.8ghz i7 [2 Core], 4GB RAM, SSD).

## GenMetrics + Synchronous Calls

Before diving in to the benchmarks themselves it is essential that we highlight one key runtime behaviour related to the `gen_metrics.` library. By default, `gen_metrics` does not monitor synchronous (blocking) calls on `GenServer` or `GenStage` applications. However, synchronous call monitoring can be activated. When activated, significant runtime costs _may_ be incurred by the application being monitored if the volume of synchronous calls is high.

What exactly do we mean by a high volume of synchronous calls? See the discussion of results for each of the following benchmark tests for further details.

## GenServer Benchmarks

### bench_cluster.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 70.0s

Benchmarking traced---server [ call ]...
Benchmarking untraced-server [ call ]...

Name                               ips        average  deviation         median
traced---server [ call ]          0.23         4.40 s     ±1.13%         4.39 s
untraced-server [ call ]          0.23         4.41 s     ±1.22%         4.41 s

Comparison: 
traced---server [ call ]          0.23
untraced-server [ call ]          0.23 - 1.00x slower
```

### bench_cluster_sync.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 70.0s

Benchmarking traced---server [ call ]...
Benchmarking untraced-server [ call ]...

Name                               ips        average  deviation         median
untraced-server [ call ]          0.20         4.89 s    ±18.49%         4.47 s
traced---server [ call ]        0.0932        10.73 s     ±6.76%        11.25 s

Comparison: 
untraced-server [ call ]          0.20
traced---server [ call ]        0.0932 - 2.19x slower
```


### bench_cluster_flow.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 210.0s

Benchmarking traced---server [ call ]...
Benchmarking traced---server [ cast ]...
Benchmarking traced---server [ info ]...
Benchmarking untraced-server [ call ]...
Benchmarking untraced-server [ cast ]...
Benchmarking untraced-server [ info ]...

Name                               ips        average  deviation         median
traced---server [ cast ]        0.0500        20.00 s     ±0.01%        20.00 s
untraced-server [ info ]        0.0500        20.00 s     ±0.00%        20.00 s
untraced-server [ cast ]        0.0500        20.00 s     ±0.01%        20.00 s
traced---server [ info ]        0.0500        20.00 s     ±0.00%        20.00 s
untraced-server [ call ]        0.0500        20.01 s     ±0.00%        20.01 s
traced---server [ call ]        0.0499        20.02 s     ±0.06%        20.02 s

Comparison: 
traced---server [ cast ]        0.0500
untraced-server [ info ]        0.0500 - 1.00x slower
untraced-server [ cast ]        0.0500 - 1.00x slower
traced---server [ info ]        0.0500 - 1.00x slower
untraced-server [ call ]        0.0500 - 1.00x slower
traced---server [ call ]        0.0499 - 1.00x slower
```


## GenStage Benchmarks

### bench_pipeline.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 140.0s

Benchmarking traced---pipeline [max_demand:    1]...
Benchmarking traced---pipeline [max_demand: 1000]...
Benchmarking untraced-pipeline [max_demand:    1]...
Benchmarking untraced-pipeline [max_demand: 1000]...

Name                                           ips        average  deviation         median
untraced-pipeline [max_demand: 1000]        0.0321        31.20 s     ±0.00%        31.20 s
untraced-pipeline [max_demand:    1]        0.0300        33.37 s     ±0.00%        33.37 s
traced---pipeline [max_demand: 1000]        0.0299        33.40 s     ±0.00%        33.40 s
traced---pipeline [max_demand:    1]        0.0156        64.22 s     ±0.00%        64.22 s

Comparison: 
untraced-pipeline [max_demand: 1000]        0.0321
untraced-pipeline [max_demand:    1]        0.0300 - 1.07x slower
traced---pipeline [max_demand: 1000]        0.0299 - 1.07x slower
traced---pipeline [max_demand:    1]        0.0156 - 2.06x slower
```


### bench_pipeline_sync.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 140.0s

Benchmarking traced---pipeline [max_demand:    1]...
Benchmarking traced---pipeline [max_demand: 1000]...
Benchmarking untraced-pipeline [max_demand:    1]...
Benchmarking untraced-pipeline [max_demand: 1000]...

Name                                           ips        average  deviation         median
untraced-pipeline [max_demand: 1000]        0.0260        38.45 s     ±0.00%        38.45 s
untraced-pipeline [max_demand:    1]        0.0229        43.69 s     ±0.00%        43.69 s
traced---pipeline [max_demand: 1000]        0.0204        48.95 s     ±0.00%        48.95 s
traced---pipeline [max_demand:    1]        0.0132        75.89 s     ±0.00%        75.89 s

Comparison: 
untraced-pipeline [max_demand: 1000]        0.0260
untraced-pipeline [max_demand:    1]        0.0229 - 1.14x slower
traced---pipeline [max_demand: 1000]        0.0204 - 1.27x slower
traced---pipeline [max_demand:    1]        0.0132 - 1.97x slower
```

## GenMetrics + BEAM Garbage Collection

xxx
By default, summary metrics, no problem

If detailed statistical metrics are enabled, equivlanet to activating a `statsd agent` directly within the BEAM. Therefore each metrics `window` in time this agent collects metrics data, the amount of which is directly propertional to the number of calls being monitored within your cluster or pipeline. Put another way, the size of the aggregated data can be very significant. And while the data is discared everytime a new `window` begins these spikes in data usage can and likely will cause the BEAM GC to run frequently in order to reclaim the unused space. 

So this is why `gen_metrics` has added support for pushing metrics data directly to a real `statsd agent` using the statistics: :statsd or statistics: :datadog options. When used, `gen_metrics` maintains only summary metrics data in-memory and the GC issue describe above is no longer triggered.

