## GenMetrics Runtime Performance Benchmarks

For those of you curious about the performance impact `gen_metrics` has on the servers and pipelines it is monitoring, we've put together a number of benchmarks to compare the overhead of traced vs untraced servers and pipelines. You can tweak and run the benchmarks yourself, they are found in the current directory.

The following sections of this document introduce each of the available benchmark tests. We examine the results and explain the implications of those results in each case. The benchmark reports that follow were generated by running the benchmarks on a 2011 Macbook Air (1.8ghz i7 [2 Core], 4GB RAM, SSD). All benchmarks are implmented and run using the [benchee benchmark](https://github.com/PragTob/benchee) library.

## GenMetrics Runtime Performance Summary

When GenMetrics is activated, varying degress of runtime overhead *may* be incurred by the application being monitored if the rate of GenServer or GenStage calls within the application is sufficiently high. So the obvious question becomes, what exactly do we mean by a sufficiently high rate of calls? The discussion of results for each of the following benchmark tests explores this topic in detail.

## GenMetrics + Synchronous Calls

Before diving in to the benchmarks themselves it is important to highlight one key runtime behaviour related to the GenMetrics library. By default, GenMetrics does not monitor synchronous (blocking) calls on `GenServer` or `GenStage` applications. However, synchronous call monitoring can be activated, using the `synchronous: true` option.

## GenServer Benchmarks

The following set of benchmark are designed to test and measure the runtime impact of GenMetrics on a simple GenServer application. Benchmark specific context is provided in each case along with an analysis of the results.

### GenServer Benchmark 1. bench_cluster.exs

This benchmark runs the following two tests:

1. traced-server [ call ]
2. untraced-server [ call ]

Both tests attempt to push as many messages as possible to a GenServer process using the synchronous (blocking) `GenServer.call/3` function. These tests each run for approximately 30 seconds. The server process within the `traced-server` test is being monitored by GenMetrics. The server process within the `untraced-server` test is not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 70.0s

Benchmarking traced---server [ call ]...
Benchmarking untraced-server [ call ]...

Name                               ips        average  deviation         median
traced---server [ call ]          0.23         4.40 s     ±1.13%         4.39 s
untraced-server [ call ]          0.23         4.41 s     ±1.22%         4.41 s

Comparison: 
traced---server [ call ]          0.23
untraced-server [ call ]          0.23 - 1.00x slower
```

On our test hardware, both tests managed to push approximately 4.5 million messages to their respective GenServer processes within the 30 second test window. That's approximately 150k messages per second.

The results indicate that zero runtime overhead was introduced by the GenMetrics library. This is easily explained by the information provided in the [GenMetrics + Synchronous Calls](#genmetrics--synchronous-calls) section above. By default, synchronous calls are not monitored by the GenMetrics library. This benchmark ran using the default configuration. Therefore GenMetrics did no real work at runtime. And so imposed no runtime overhead as collaborated by the test results shown here.

### GenServer Benchmark 2. bench_cluster_sync.exs

This benchmark runs the following two tests:

1. traced-server [ call ]
2. untraced-server [ call ]

Both tests attempt to push as many messages as possible to a GenServer process using the synchronous (blocking) `GenServer.call/3` function. These tests each run for approximately 30 seconds. The server process within the `traced-server` test is being monitored by GenMetrics. The `synchronous: true` option has been enabled for this process. The server process within the `untraced-server` test is not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 70.0s

Benchmarking traced---server [ call ]...
Benchmarking untraced-server [ call ]...

Name                               ips        average  deviation         median
untraced-server [ call ]          0.20         4.89 s    ±18.49%         4.47 s
traced---server [ call ]        0.0932        10.73 s     ±6.76%        11.25 s

Comparison: 
untraced-server [ call ]          0.20
traced---server [ call ]        0.0932 - 2.19x slower
```

On our test hardware, the `traced-server` test managed to push approximately 2.0 million messages to its GenServer processes within the 30 second test window. That's approximately 67k messages per second. The `untraced-server` test managed to push approximately 4.5 million messages to its process within the same window. That's approrximately 150k message per second.

The results indicate a significant runtime overhead introduced by the GenMetrics library. By default, synchronous calls are not monitored by the GenMetrics library. However, this benchmark activated monitoring for synchronous calls. As indicated by the results the test using the monitored server performed `2.19x slower`. We can directly attribute this slowdown to the runtime overhead introduced by the GenMetrics library.

Keep in mind, on our test hardware the GenMetrics tracing agent monitored 2.0 million synchronous calls in 30 seconds. This is a good example of what we described as `a high rate of synchronous calls` in the [GenMetrics Runtime Performance Summary](#genmetrics-runtime-performance-summary) section.

### GenServer Benchmark 3. bench_cluster_flow.exs

The result of the previous benchmark might suggest that you would never want to activate monitoring for synchronous calls. But is that really true? This benchmark attempts to help answer that question by running the following tests:

1. traced-server [ call ]
2. traced-server [ cast ]
3. traced-server [ info ]
1. untraced-server [ call ]
2. untraced-server [ cast ]
3. untraced-server [ info ]

Unlike previous GenServer tests, these tests do not attempt to pass as many messages as possible to the GenServer processes. Instead, these tests simulate a steady flow of 1000 messages per second. These tests each run for approximately 30 seconds. The server process within the `traced-server` tests is being monitored by GenMetrics. The `synchronous: true` option has also been enabled for this process. The server process within the `untraced-server` tests is not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 210.0s

Benchmarking traced---server [ call ]...
Benchmarking traced---server [ cast ]...
Benchmarking traced---server [ info ]...
Benchmarking untraced-server [ call ]...
Benchmarking untraced-server [ cast ]...
Benchmarking untraced-server [ info ]...

Name                               ips        average  deviation         median
traced---server [ cast ]        0.0500        20.00 s     ±0.01%        20.00 s
untraced-server [ info ]        0.0500        20.00 s     ±0.00%        20.00 s
untraced-server [ cast ]        0.0500        20.00 s     ±0.01%        20.00 s
traced---server [ info ]        0.0500        20.00 s     ±0.00%        20.00 s
untraced-server [ call ]        0.0500        20.01 s     ±0.00%        20.01 s
traced---server [ call ]        0.0499        20.02 s     ±0.06%        20.02 s

Comparison: 
traced---server [ cast ]        0.0500
untraced-server [ info ]        0.0500 - 1.00x slower
untraced-server [ cast ]        0.0500 - 1.00x slower
traced---server [ info ]        0.0500 - 1.00x slower
untraced-server [ call ]        0.0500 - 1.00x slower
traced---server [ call ]        0.0499 - 1.00x slower
```

All tests managed to push 1000 messages per second to their respective GenServer processes for the duration of the 30 second test window. The test results indicate that no perceivable runtime overhead was incurred by the server process being monitored by GenMetrics. This is true both for synchronous `GenServer.call/3` calls and asynchronous `GenServer.cast/2` and `Kernel.send/2` calls.

Combined with what we learned in the previous benchmark test we can now make the following generalization:

> A high rate of calls (synchronous or asynchronous) per second is likely to have significant impact on the runtime performance of an application being monitored by GenMetrics. A low or moderate rate of calls (synchronous or asynchronous) per second is likely to have little or zero impact on the runtime performance of an application being monitored by GenMetrics.

On our hardware, the a high rate of calls (67k calls/s) was sufficiently high to see significant runtime impact. While a low rate of calls (1k calls/s) was so low that zero runtime impact was observed regardless of whether we were monitoring synchronous or asynchronous calls.

#### So at what point will GenMetrics have a negative impact on the runtime performance of your application?

The answer is, it depends. It depends entirely on the nature of your applications architecture, on the nature of the messages passing to your server or through your pipeline, and on the hardware resources on which your application is deployed.

The best way to find out what a `safe-rate` means on your hardware is to enable GenMetrics and experience actual runtime behaviour. We strongly recommend doing such experimentation in development or staging environments only, never in production environments.

## GenStage Benchmarks

The following set of benchmark are designed to test and measure the runtime impact of `gen_metrics` on a simple GenStage pipeline application. Benchmark specific context is provided in each case along with an analytis of the results.

### GenStage Benchmark 1. bench_pipeline.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 140.0s

Benchmarking traced---pipeline [max_demand:    1]...
Benchmarking traced---pipeline [max_demand: 1000]...
Benchmarking untraced-pipeline [max_demand:    1]...
Benchmarking untraced-pipeline [max_demand: 1000]...

Name                                           ips        average  deviation         median
untraced-pipeline [max_demand: 1000]        0.0321        31.20 s     ±0.00%        31.20 s
untraced-pipeline [max_demand:    1]        0.0300        33.37 s     ±0.00%        33.37 s
traced---pipeline [max_demand: 1000]        0.0299        33.40 s     ±0.00%        33.40 s
traced---pipeline [max_demand:    1]        0.0156        64.22 s     ±0.00%        64.22 s

Comparison: 
untraced-pipeline [max_demand: 1000]        0.0321
untraced-pipeline [max_demand:    1]        0.0300 - 1.07x slower
traced---pipeline [max_demand: 1000]        0.0299 - 1.07x slower
traced---pipeline [max_demand:    1]        0.0156 - 2.06x slower
```


### GenStage Benchmark 2. bench_pipeline_sync.exs

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 140.0s

Benchmarking traced---pipeline [max_demand:    1]...
Benchmarking traced---pipeline [max_demand: 1000]...
Benchmarking untraced-pipeline [max_demand:    1]...
Benchmarking untraced-pipeline [max_demand: 1000]...

Name                                           ips        average  deviation         median
untraced-pipeline [max_demand: 1000]        0.0260        38.45 s     ±0.00%        38.45 s
untraced-pipeline [max_demand:    1]        0.0229        43.69 s     ±0.00%        43.69 s
traced---pipeline [max_demand: 1000]        0.0204        48.95 s     ±0.00%        48.95 s
traced---pipeline [max_demand:    1]        0.0132        75.89 s     ±0.00%        75.89 s

Comparison: 
untraced-pipeline [max_demand: 1000]        0.0260
untraced-pipeline [max_demand:    1]        0.0229 - 1.14x slower
traced---pipeline [max_demand: 1000]        0.0204 - 1.27x slower
traced---pipeline [max_demand:    1]        0.0132 - 1.97x slower
```

## GenMetrics + BEAM Garbage Collection

By default, summary metrics, no problem

If detailed statistical metrics are enabled, equivlanet to activating a `statsd agent` directly within the BEAM. Therefore each metrics `window` in time this agent collects metrics data, the amount of which is directly propertional to the number of calls being monitored within your cluster or pipeline. Put another way, the size of the aggregated data can be very significant. And while the data is discared everytime a new `window` begins these spikes in data usage can and likely will cause the BEAM GC to run frequently in order to reclaim the unused space. 

So this is why `gen_metrics` has added support for pushing metrics data directly to a real `statsd agent` using the statistics: :statsd or statistics: :datadog options. When used, `gen_metrics` maintains only summary metrics data in-memory and the GC issue describe above is no longer triggered.
