## GenMetrics Runtime Performance Benchmarks

For those of you curious about the performance impact `gen_metrics` has on the servers and pipelines it is monitoring, we've put together a number of benchmarks to compare the overhead of traced vs untraced servers and pipelines. You can tweak and run the benchmarks yourself from the project root directory.

The following sections introduce each of the available benchmark tests. We examine the results and explain the implications of those results in each case. The benchmark reports that follow were generated by running the benchmarks on a 2011 Macbook Air (1.8ghz i7 [2 Core], 4GB RAM, SSD). All benchmarks are implmented and run using the [benchee benchmark](https://github.com/PragTob/benchee) library.

## GenMetrics Runtime Performance Summary

When GenMetrics is activated, varying degress of runtime overhead *may* be incurred by the application being monitored if the rate of GenServer or GenStage calls within the application is sufficiently high. So the obvious next question becomes, what exactly do we mean by a sufficiently high rate of calls? The discussion of results for each of the following benchmark tests explores this topic in detail.

## GenMetrics + Synchronous Calls

Before diving in to the benchmarks themselves it is important to highlight one key runtime behaviour related to the GenMetrics library. By default, GenMetrics does not monitor synchronous (blocking) calls on `GenServer` or `GenStage` applications. However, synchronous call monitoring can be activated, using the `synchronous: true` option.

## GenServer Benchmarks

The following set of benchmark are designed to test and measure the runtime impact of GenMetrics on a simple GenServer application. Benchmark specific context is provided in each case along with an analysis of the results.

### GenServer Benchmark 1. bench_cluster.exs

```
mix bench/bench_cluster
```

This benchmark runs the following tests:

1. traced-server [ call ]
2. untraced-server [ call ]

Both tests attempt to push as many messages as possible to a GenServer process using the synchronous (blocking) `GenServer.call/3` function. These tests each run for approximately 30 seconds. The server process within the `traced-server` test is being monitored by GenMetrics. The server process within the `untraced-server` test is not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 70.0s

Benchmarking traced---server [ call ]...
Benchmarking untraced-server [ call ]...

Name                               ips        average  deviation         median
traced---server [ call ]          0.23         4.40 s     ±1.13%         4.39 s
untraced-server [ call ]          0.23         4.41 s     ±1.22%         4.41 s

Comparison: 
traced---server [ call ]          0.23
untraced-server [ call ]          0.23 - 1.00x slower
```

On our test hardware, both tests managed to push approximately 4.5 million messages to their respective GenServer processes within the 30 second test window. That's approximately 150k messages-per-second.

The results indicate that zero runtime overhead was introduced by the GenMetrics library. This is easily explained by the information provided in the [GenMetrics + Synchronous Calls](#genmetrics--synchronous-calls) section above. By default, synchronous calls are not monitored by the GenMetrics library. This benchmark ran using the default configuration. Therefore GenMetrics did no real work at runtime. And so imposed no runtime overhead as collaborated by the test results shown here.

### GenServer Benchmark 2. bench_cluster_sync.exs

```
mix bench/bench_cluster_sync
```

This benchmark runs the following tests:

1. traced-server [ call ]
2. untraced-server [ call ]

Both tests attempt to push as many messages as possible to a GenServer process using the synchronous (blocking) `GenServer.call/3` function. These tests each run for approximately 30 seconds. The server process within the `traced-server` test is being monitored by GenMetrics. The `synchronous: true` option has been enabled for this process. The server process within the `untraced-server` test is not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 70.0s

Benchmarking traced---server [ call ]...
Benchmarking untraced-server [ call ]...

Name                               ips        average  deviation         median
untraced-server [ call ]          0.20         4.89 s    ±18.49%         4.47 s
traced---server [ call ]        0.0932        10.73 s     ±6.76%        11.25 s

Comparison: 
untraced-server [ call ]          0.20
traced---server [ call ]        0.0932 - 2.19x slower
```

On our test hardware, the `traced-server` test managed to push approximately 2.0 million messages to its GenServer processes within the 30 second test window. That's approximately 67k messages-per-second. The `untraced-server` test managed to push approximately 4.5 million messages to its process within the same window. That's approrximately 150k message per second.

The results indicate a significant runtime overhead introduced by the GenMetrics library. By default, synchronous calls are not monitored by the GenMetrics library. However, this benchmark activated monitoring for synchronous calls. As indicated by the results the test using the monitored server performed `2.19x slower`. We can directly attribute this slowdown to the runtime overhead introduced by the GenMetrics library.

Keep in mind, on our test hardware the GenMetrics tracing agent monitored 2.0 million synchronous calls in 30 seconds. For our specific test hardware, this is a good example of what we described as `a high rate of synchronous calls` in the [GenMetrics Runtime Performance Summary](#genmetrics-runtime-performance-summary) section.

### GenServer Benchmark 3. bench_cluster_flow.exs

```
mix bench/bench_cluster_flow
```

The result of the previous benchmark might suggest that you would never want to activate GenMetrics monitoring at all. But is that really true? This benchmark attempts to help answer that question by running the following tests:

1. traced-server [ call ]
2. traced-server [ cast ]
3. traced-server [ info ]
1. untraced-server [ call ]
2. untraced-server [ cast ]
3. untraced-server [ info ]

Unlike previous GenServer tests, these tests do not attempt to flood the GenServer processes with as many messages as possible. Instead, these tests simulate a steady flow of 1000 messages-per-second. These tests each run for approximately 30 seconds. The server process within the `traced-server` tests is being monitored by GenMetrics. The `synchronous: true` option has also been enabled for this process. The server process within the `untraced-server` tests is not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 210.0s

Benchmarking traced---server [ call ]...
Benchmarking traced---server [ cast ]...
Benchmarking traced---server [ info ]...
Benchmarking untraced-server [ call ]...
Benchmarking untraced-server [ cast ]...
Benchmarking untraced-server [ info ]...

Name                               ips        average  deviation         median
traced---server [ cast ]        0.0500        20.00 s     ±0.01%        20.00 s
untraced-server [ info ]        0.0500        20.00 s     ±0.00%        20.00 s
untraced-server [ cast ]        0.0500        20.00 s     ±0.01%        20.00 s
traced---server [ info ]        0.0500        20.00 s     ±0.00%        20.00 s
untraced-server [ call ]        0.0500        20.01 s     ±0.00%        20.01 s
traced---server [ call ]        0.0499        20.02 s     ±0.06%        20.02 s

Comparison: 
traced---server [ cast ]        0.0500
untraced-server [ info ]        0.0500 - 1.00x slower
untraced-server [ cast ]        0.0500 - 1.00x slower
traced---server [ info ]        0.0500 - 1.00x slower
untraced-server [ call ]        0.0500 - 1.00x slower
traced---server [ call ]        0.0499 - 1.00x slower
```

All tests managed to push 1000 messages-per-second to their respective GenServer processes for the duration of the 30 second window. The test results indicate that no perceivable runtime overhead was incurred by the server process being monitored by GenMetrics. This is true both for synchronous `GenServer.call/3` calls and asynchronous `GenServer.cast/2` and `Kernel.send/2` calls.

Combined with what we learned in the previous benchmark test we can now make the following generalization:

> A genuinely high rate of calls (synchronous or asynchronous) per second is likely to have negative impact on the runtime performance of an application being monitored by GenMetrics. A low or moderate rate of calls (synchronous or asynchronous) per second is likely to have little or zero impact on the runtime performance of an application being monitored by GenMetrics.

On our test hardware, 67k calls-per-second was sufficiently high to see significant runtime impact. While just 1k calls-per-second was so low that zero runtime impact was observed regardless of whether we were monitoring synchronous or asynchronous calls.

#### So at what point will GenMetrics have a negative impact on the runtime performance of your application?

The answer is, it depends. Primarily it depends on the `rate-of-calls` being handled by your application and, like any application, on the hardware on which your application is deployed.

The best way to find out what a `safe-rate` means on your hardware is to enable GenMetrics and experience actual runtime behaviour. We strongly recommend doing such experimentation in development or staging environments only, and never in production environments.

## GenStage Benchmarks

The following set of benchmark are designed to test and measure the runtime impact of `gen_metrics` on a simple GenStage pipeline application. Benchmark specific context is provided in each case along with an analytis of the results.

### GenStage Benchmark 1. bench_pipeline.exs

```
mix bench/bench_pipeline
```

This benchmark runs the following tests:

1. traced-pipeline [ max_demand:    1 ]
2. traced-pipeline [ max_demand: 1000 ]
3. untraced-pipeline [ max_demand:    1 ]
4. untraced-pipeline [ max_demand: 1000 ]

Each test attempts to push as many messages as possible through a GenStage pipeline. These tests each run for approximately 30 seconds. The GenStage processes within the `traced-pipeline` test are being monitored by GenMetrics. The GenStage processes within the untraced-server test are not being monitored by GenMetrics.

```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 140.0s

Benchmarking traced---pipeline [max_demand:    1]...
Benchmarking traced---pipeline [max_demand: 1000]...
Benchmarking untraced-pipeline [max_demand:    1]...
Benchmarking untraced-pipeline [max_demand: 1000]...

Name                                           ips        average  deviation         median
untraced-pipeline [max_demand: 1000]        0.0321        31.20 s     ±0.00%        31.20 s
untraced-pipeline [max_demand:    1]        0.0300        33.37 s     ±0.00%        33.37 s
traced---pipeline [max_demand: 1000]        0.0299        33.40 s     ±0.00%        33.40 s
traced---pipeline [max_demand:    1]        0.0156        64.22 s     ±0.00%        64.22 s

Comparison: 
untraced-pipeline [max_demand: 1000]        0.0321
untraced-pipeline [max_demand:    1]        0.0300 - 1.07x slower
traced---pipeline [max_demand: 1000]        0.0299 - 1.07x slower
traced---pipeline [max_demand:    1]        0.0156 - 2.06x slower
```

On our test hardware, all tests except `traced-pipeline [max_demand: 1]` managed to push approximately 2.0 million messages through their respective GenStage pipelines within the 30 second test window. That's approximately 67k messages-per-second. However, the `traced-pipeline [max_demand: 1]` test only managed to push approximately 1.0 million message through its GenStage pipeline. How can we explain this difference?

The answer is simple as soon as you understand the difference between the `rate-of-throughput` and the `rate-of-calls` within a pipeline.

To understand those different rates, lets compare the results for `untraced-pipeline [max_demand: 1000]` and `traced-pipeline [max_demand: 1000]`. The `max_demand` value indicated is in fact the default value for a GenStage pipeline. Notice how there is very low runtime overhead on the `traced-pipeline [max_demand: 1000]`, just `1.07 times` slower than the untraced pipeline. Considering the test pushed 67k messages-per-second this result indicates the GenMetrics runtime impact was neglible.

Thanks to GenStage intelligently batching events to satisfy upstream demand the `rate-of-throughput` is high while the `rate-of-calls` to deliver that throughput is low. Specifically, in this test where `max_demand` was set to 1000, the rate-of-throughput was 67k messages-per-second while the rate-of-calls per stage in the pipeline was just 134 calls-per-second (67k / 500 = 134).

Now lets compare the results for the `untraced-pipeline [max_demand: 1]` and `traced_pipeline [max_demand: 1]` tests. In this case we see a significant impact on runtime performance, reported as approximately `2.06x slower`. In these tests because `max_demand` was constrained to one, GenStage could not perform any batching of events to statisfy upstream demand. Instead, every individual message was sent upstream one event at a time. So while the rate-of-throughput stayed the same for the `untraced-pipeline [max_demand: 1]`, the rate-of-calls increased from 134 calls-per-second to 67k calls-per-second per stage in the pipeline. It is this massive jump in the rate-of-calls that explains the significant slowdown in runtime performance of the `traced-pipeline [max_demand: 1]` test.


### GenStage Benchmark 2. bench_pipeline_sync.exs

```
mix bench/bench_pipeline_sync
```

This benchmark runs the following tests:

1. traced-pipeline [ max_demand:    1 ]
2. traced-pipeline [ max_demand: 1000 ]
3. untraced-pipeline [ max_demand:    1 ]
4. untraced-pipeline [ max_demand: 1000 ]

Each test attempts to push as many messages as possible through a GenStage pipeline. These tests each run for approximately 30 seconds. The GenStage processes within the `traced-pipeline` test are being monitored by GenMetrics. The `synchronous: true` option has been enabled for these process. The GenStage processes within the untraced-server test are not being monitored by GenMetrics.


```
Elixir 1.4.1
Erlang 19.2
Benchmark suite executing with the following configuration:
warmup: 5.0s
time: 30.0s
parallel: 1
inputs: none specified
Estimated total run time: 140.0s

Benchmarking traced---pipeline [max_demand:    1]...
Benchmarking traced---pipeline [max_demand: 1000]...
Benchmarking untraced-pipeline [max_demand:    1]...
Benchmarking untraced-pipeline [max_demand: 1000]...

Name                                           ips        average  deviation         median
untraced-pipeline [max_demand: 1000]        0.0260        38.45 s     ±0.00%        38.45 s
untraced-pipeline [max_demand:    1]        0.0229        43.69 s     ±0.00%        43.69 s
traced---pipeline [max_demand: 1000]        0.0204        48.95 s     ±0.00%        48.95 s
traced---pipeline [max_demand:    1]        0.0132        75.89 s     ±0.00%        75.89 s

Comparison: 
untraced-pipeline [max_demand: 1000]        0.0260
untraced-pipeline [max_demand:    1]        0.0229 - 1.14x slower
traced---pipeline [max_demand: 1000]        0.0204 - 1.27x slower
traced---pipeline [max_demand:    1]        0.0132 - 1.97x slower
```

On our test hardware, the `untraced-pipeline` tests managed to push approximately 2.0 million messages through the GenServer pipeline within the 30 second test window. That's approximately 67k messages-per-second. The `traced-server [max_demand: 500]` test achieved slightly lower throughput numbers, reported as being `1.27x slower`. And the `traced-server [max_demand: 1]` test performed significantly slower, reported as `1.97x slower`. How can we explain these numbers?

Again, the answer is revealed by the differences in the `rate-of-calls` within the pipeline and how those differences impact on GenMetrics runtime overhead, which in turn impacts on the `rate-of-throughput`.

For the `traced-pipeline` tests the `synchronous: true` option was activated. Therefore, the `GenStage.call/3` function used to push each of the 2.0 million messages into the GenStage producer for the pipeline were monitored. This significantly increased the `rate-of-calls` being monitored by GenMetrics. The `traced-pipeline [max-demand: 1]` test also suffered from a high rate-of-calls within its pipeline for all of the reasons detailed in the previous benchmark.

When considering whether to enabled GenMetrics monitoring on your pipeline a good rule of thumb is to consider the likely `rate-of-calls` that your pipeline will experience. The best way to find out what a `safe-rate` means on your hardware is to enable GenMetrics and experience actual runtime behaviour. We strongly recommend doing such experimentation in development or staging environments only, and never in production environments.

## GenMetrics + BEAM Garbage Collection

By default, when GenMetrics is enabled is collects and report only summary metrics data. This type of metrics data collection has very little runtime overhead in terms of memory usage and should never trigger spikes in memory usage or GC.

If detailed statistical metrics are activated using the `statistics: true` option, significant amounts of metrics data are collected. Activating this feature is a lot like activating a `statsd agent` directly within the BEAM. The exact amount of data collected is directly proportional to the `rate-of-calls` discussed above. It is therefore strongly recommended that this feature only be enabled in environments where the `rate-of-calls` is known to be low. Otherwise, spiked memory usage and frequent GC will occur.

If the type of insights provided by statistical metrics are needed then we strongly recommend using the existing support for redirecting metrics data to an external `statsd` agent. This can be achieved using the `statistics: :statsd` and `statistics: :datadog` options. Just remember to adjust the `sample_rate` for these agents so you do not overwhelm their ability to collect and report metrics. When using these external `statsd` agents GenMetrics incurs very little runtime overhead in terms of memory usage and should never trigger spikes in memory usage or GC.

